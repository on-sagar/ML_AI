{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#one neuron with four inputs and one output\n",
    "\n",
    "inputs=[1,2,3,2.5]\n",
    "weights=[0.2,0.8,-0.5,1.0]\n",
    "bias=2\n",
    "\n",
    "\n",
    "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 3.985]\n"
     ]
    }
   ],
   "source": [
    "#three neurons with four inputs and each have single output\n",
    "\n",
    "weights1=[0.2,0.8,-0.5,1.0]\n",
    "weights2=[0.5,-0.91,0.26,-0.5]\n",
    "weights3=[0.26,0.27,0.17,0.87]\n",
    "\n",
    "bias1=2\n",
    "bias2=3\n",
    "bias3=0.5\n",
    "\n",
    "#outputs from three neurons\n",
    "output= [inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+bias1,\n",
    "         inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+bias2,\n",
    "         inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+bias3]\n",
    "print(output)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   0.21  3.885]\n"
     ]
    }
   ],
   "source": [
    "##a more compact technique\n",
    "inputs=[1,2,3,2.5]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1.0],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "# layer_outputs=[]\n",
    "# for neuron_weights,neuron_bias in zip(weights,biases): #zip combines two lists element-wise\n",
    "#     neuron_output=0\n",
    "#     for n_input,weight in zip(inputs,neuron_weights):\n",
    "#         neuron_output+=n_input*weight\n",
    "#     neuron_output+=neuron_bias\n",
    "#     layer_outputs.append(neuron_output)\n",
    "# # print(layer_outputs)\n",
    "\n",
    "\n",
    "'''DOT PRODUCT'''\n",
    "'''USED TO MULTIPLY ELMENT WISE INPUTS AND BIASES IN NN CALCULATIONS'''\n",
    "# inputs=[1,2,3,2.5]\n",
    "# weights=[0.2,0.8,-0.5,1.0]\n",
    "# bias=2\n",
    "\n",
    "output=np.dot(weights,inputs)+bias  #element wise multiplication meaning weights[0]*input + bias and simliar\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "'''BATCH'''\n",
    "inputs=[[1,2,3,2.5],\n",
    "        [2.0,5.0,-1.0,2.0],\n",
    "        [-1.5,2.7,3.3,-0.8]]\n",
    "weights=[[0.2,0.8,-0.5,1.0],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]] \n",
    "biases=[2,3,0.5]\n",
    "weights2=[[0.1,-0.14,0.5],\n",
    "         [-0.5,0.12,-0.33],\n",
    "         [-0.44,0.73,-0.13]] \n",
    "biases2=[-1,2,-0.5]\n",
    "#layer 1 has 4 inputs 3 outputs and layer 2 has 3 inputs from layer 1 outputs and 3 outputs\n",
    "#the number of nodes(uniques sets of weights and biases) define the number of outputs\n",
    "#clearly \n",
    "layer1_outputs=np.dot(inputs,np.array(weights).T)+biases\n",
    "layer2_outputs=np.dot(layer1_outputs,np.array(weights2).T)+biases2\n",
    "\n",
    "print(layer2_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"OBJECTS\"\"\"\n",
    "\n",
    "#input data to out neural network\n",
    "#we have three samples\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "#the shape here is the nuumber of neurons x no. of inputs\n",
    "#we have three inputs here\n",
    "#each input is made up of 4 independent variables\n",
    "X= [[1,2,3,2.5],\n",
    "    [2.0,5.0,-1.0,2.0],\n",
    "    [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights=0.1*np.random.randn(n_inputs,n_neurons) #this is reversed to eliminate transpose later on\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "layer1=Layer_Dense(4,5)\n",
    "layer2=Layer_Dense(5,2) #regardless of these values the number of rows is gonna be same in each output because it denotes the number of samples, only change is column number\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "\n",
    "###if you don't understand it at once, think deeply and give time, you had once understood this so its just a matter of time and effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs=[0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output=[]\n",
    "\n",
    "#Rectified linear function\n",
    "for i in inputs:\n",
    "    # if i > 0:\n",
    "    #     output.append(i)\n",
    "    # elif i<=0:\n",
    "    #     output.append(0)\n",
    "    output.append(max(0,i))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.00080274, 0.        , 0.00065987, 0.        , 0.00083796],\n",
       "       [0.00082768, 0.        , 0.0005103 , 0.        , 0.00454176],\n",
       "       ...,\n",
       "       [0.01800099, 0.        , 0.00336391, 0.        , 0.26602511],\n",
       "       [0.        , 0.16360456, 0.        , 0.13192221, 0.20328078],\n",
       "       [0.        , 0.        , 0.        , 0.05579987, 0.31271706]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACTIVATION FUNCTIONS\n",
    "\n",
    "def create_data(points,classes):\n",
    "    X=np.zeros((points*classes,2))\n",
    "    y=np.zeros(points*classes,dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix=range(points*class_number,points*(class_number+1))\n",
    "        r=np.linspace(0.0,1,points)\n",
    "        t=np.linspace(class_number*4,(class_number+1)*4,points)+np.random.randn(points)*0.2\n",
    "        X[ix]=np.c_[r*np.sin(t*2.5),r*np.cos(t*2.5)]\n",
    "        y[ix]=class_number\n",
    "    return X,y\n",
    "X,y=create_data(100,3)\n",
    "# for class_number in range(3):\n",
    "#     class_indices = np.where(y == class_number)[0]\n",
    "#     plt.scatter(X[class_indices, 0], X[class_indices, 1], label=f'Class {class_number}')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "#now we are gonna deal with the layers in the neural network\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights=0.1*np.random.randn(n_inputs,n_neurons) #this is reversed to eliminate transpose later on\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.output=np.maximum(0,inputs)\n",
    "\n",
    "layer1=Layer_Dense(2,5)\n",
    "activation1=Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "# layer1.output\n",
    "activation1.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "[[1.21510418e+02]\n",
      " [7.33197354e+03]\n",
      " [4.09595540e+00]]\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "'''SOFTMAX ACTIVATION FUNCTION'''\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "layeroutputs=[[4.8,1.21,2.385],\n",
    "              [8.9,-1.81,0.2],\n",
    "              [1.41,1.051,0.026]]\n",
    "\n",
    "exp_values=np.exp(layeroutputs)\n",
    "print(exp_values)\n",
    "# New_exp_values=np.max(exp_values,axis=1,keepdims=True)\n",
    "# print(New_exp_values)\n",
    "# print(exp_values)\n",
    "\n",
    "\n",
    "norm_values=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "print(norm_values)\n",
    "\n",
    "# print(norm_values)\n",
    "# print(sum(norm_values))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33335766 0.33332326 0.33331908]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33329324 0.33352161 0.33318516]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33325475 0.33370211 0.33304314]\n",
      " [0.33333791 0.33333144 0.33333065]\n",
      " [0.33326499 0.3336541  0.33308091]\n",
      " [0.3332548  0.33370189 0.33304331]\n",
      " [0.3331655  0.33411999 0.33271451]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33321239 0.3339006  0.33288701]\n",
      " [0.33316321 0.33413072 0.33270608]\n",
      " [0.33318544 0.33402674 0.33278782]\n",
      " [0.33324289 0.33455247 0.33220463]\n",
      " [0.33310405 0.33440712 0.33248883]\n",
      " [0.3338309  0.33423708 0.33193202]\n",
      " [0.33346317 0.33466808 0.33186875]\n",
      " [0.33352585 0.33470242 0.33177173]\n",
      " [0.33342318 0.33485191 0.33172491]\n",
      " [0.3330005  0.3348899  0.3321096 ]\n",
      " [0.33304801 0.33516971 0.33178227]\n",
      " [0.33385056 0.33473505 0.33141439]\n",
      " [0.33407064 0.33456356 0.3313658 ]\n",
      " [0.3349522  0.33366179 0.33138601]\n",
      " [0.33359254 0.33520951 0.33119795]\n",
      " [0.33344919 0.33538655 0.33116426]\n",
      " [0.33530617 0.33348139 0.33121244]\n",
      " [0.33384508 0.33523915 0.33091577]\n",
      " [0.33583379 0.33286892 0.33129729]\n",
      " [0.33623802 0.33217115 0.33159083]\n",
      " [0.33598952 0.33285825 0.33115223]\n",
      " [0.33540985 0.33384471 0.33074544]\n",
      " [0.33614568 0.33284664 0.33100767]\n",
      " [0.33663634 0.33200975 0.33135391]\n",
      " [0.33662251 0.33216941 0.33120808]\n",
      " [0.33525605 0.33254814 0.33219581]\n",
      " [0.33622683 0.33320975 0.33056342]\n",
      " [0.33652396 0.33283869 0.33063736]\n",
      " [0.33683828 0.33238053 0.33078119]\n",
      " [0.33720474 0.3317812  0.33101406]\n",
      " [0.33597134 0.33398891 0.33003975]\n",
      " [0.33732896 0.33172749 0.33094355]\n",
      " [0.3373397  0.33172121 0.33093909]\n",
      " [0.33648301 0.33205624 0.33146075]\n",
      " [0.33565581 0.33238392 0.33196027]\n",
      " [0.33760222 0.33161511 0.33078267]\n",
      " [0.33457293 0.3328199  0.33260717]\n",
      " [0.33352849 0.33325254 0.33321897]\n",
      " [0.33548507 0.33244963 0.33206529]\n",
      " [0.33334394 0.33332894 0.33332712]\n",
      " [0.33457885 0.33281745 0.3326037 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33646062 0.33205852 0.33148086]\n",
      " [0.3339924  0.33306043 0.33294717]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33350667 0.33326157 0.33323176]\n",
      " [0.33333939 0.33333083 0.33332978]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33369057 0.33318543 0.333124  ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.3330711  0.33456088 0.33236802]\n",
      " [0.33315232 0.33418163 0.33266605]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33290162 0.33534963 0.33174874]\n",
      " [0.33286979 0.33549741 0.33163281]\n",
      " [0.33317117 0.3340935  0.33273534]\n",
      " [0.33287892 0.33545502 0.33166606]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33210172 0.33902538 0.3288729 ]\n",
      " [0.33218879 0.33862895 0.32918226]\n",
      " [0.33275514 0.33602849 0.33121637]\n",
      " [0.33197838 0.33958553 0.3284361 ]\n",
      " [0.33446535 0.33882399 0.32671066]\n",
      " [0.33207949 0.33912647 0.32879404]\n",
      " [0.33537033 0.33809383 0.32653584]\n",
      " [0.33190323 0.34034596 0.32775081]\n",
      " [0.33368156 0.33972214 0.32659631]\n",
      " [0.33394401 0.33962604 0.32642996]\n",
      " [0.33840919 0.33506807 0.32652274]\n",
      " [0.33513691 0.3387656  0.32609749]\n",
      " [0.33230272 0.34066619 0.32703109]\n",
      " [0.33237652 0.3407248  0.32689869]\n",
      " [0.33231258 0.3408238  0.32686362]\n",
      " [0.33236207 0.34088915 0.32674878]\n",
      " [0.33822124 0.33582781 0.32595095]\n",
      " [0.33550709 0.33887818 0.32561472]\n",
      " [0.33937336 0.33460584 0.3260208 ]\n",
      " [0.33779193 0.33656078 0.32564729]\n",
      " [0.33839762 0.33597566 0.32562671]\n",
      " [0.34029301 0.33365057 0.32605641]\n",
      " [0.34255767 0.32961852 0.32782382]\n",
      " [0.34220272 0.32975188 0.3280454 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33340808 0.33333229 0.33325963]\n",
      " [0.33350744 0.3332633  0.33322926]\n",
      " [0.33357923 0.33323422 0.33318655]\n",
      " [0.33367902 0.33319422 0.33312675]\n",
      " [0.33380311 0.33314516 0.33305173]\n",
      " [0.33371559 0.3331782  0.33310621]\n",
      " [0.33379933 0.33314442 0.33305625]\n",
      " [0.33371389 0.33360318 0.33268293]\n",
      " [0.33371881 0.33317514 0.33310605]\n",
      " [0.33427259 0.33295775 0.33276965]\n",
      " [0.33366669 0.33319532 0.33313799]\n",
      " [0.33394174 0.33308487 0.33297339]\n",
      " [0.33350715 0.33326138 0.33323147]\n",
      " [0.33378945 0.33314448 0.33306607]\n",
      " [0.33416772 0.33299341 0.33283887]\n",
      " [0.33376748 0.33315358 0.33307894]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33363095 0.33321012 0.33315893]\n",
      " [0.33362422 0.3332129  0.33316288]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333364 0.33333321 0.33333315]\n",
      " [0.33334039 0.33333041 0.3333292 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33329726 0.33350271 0.33320002]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33316044 0.33414367 0.33269589]\n",
      " [0.3331813  0.33404609 0.3327726 ]\n",
      " [0.33324383 0.33375331 0.33300286]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33328682 0.33355172 0.33316146]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33325239 0.33371318 0.33303443]\n",
      " [0.33306989 0.33456653 0.33236358]\n",
      " [0.33304401 0.33468721 0.33226878]\n",
      " [0.33315112 0.33418724 0.33266164]\n",
      " [0.33268224 0.33636536 0.3309524 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33266409 0.33644916 0.33088676]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33262148 0.33664564 0.33073288]\n",
      " [0.33284605 0.3356075  0.33154645]\n",
      " [0.33330514 0.33672388 0.32997099]\n",
      " [0.33254034 0.33704874 0.33041092]\n",
      " [0.33299892 0.33701669 0.32998439]\n",
      " [0.33260768 0.33670925 0.33068307]\n",
      " [0.33249201 0.3372414  0.33026659]\n",
      " [0.33378627 0.33681332 0.32940042]\n",
      " [0.33460513 0.33612482 0.32927005]\n",
      " [0.33803416 0.33144532 0.33052052]\n",
      " [0.33471621 0.33616869 0.3291151 ]\n",
      " [0.33668373 0.33403357 0.32928269]\n",
      " [0.33264272 0.33776295 0.32959433]\n",
      " [0.33331695 0.33758118 0.32910188]\n",
      " [0.33517495 0.33595804 0.32886701]\n",
      " [0.33420674 0.33711147 0.3286818 ]\n",
      " [0.33870193 0.33117673 0.33012133]\n",
      " [0.33667576 0.33460878 0.32871546]\n",
      " [0.3387575  0.33115003 0.33009247]\n",
      " [0.3385825  0.33121607 0.33020143]\n",
      " [0.33885624 0.33149645 0.32964731]\n",
      " [0.339168   0.33098853 0.32984347]\n",
      " [0.33927181 0.33095214 0.32977605]\n",
      " [0.33908119 0.33101667 0.32990214]\n",
      " [0.33901392 0.33104142 0.32994466]\n",
      " [0.33946433 0.3308661  0.32966958]\n",
      " [0.33854421 0.33122398 0.3302318 ]\n",
      " [0.33538746 0.33248221 0.33213033]\n",
      " [0.33733037 0.33170402 0.3309656 ]\n",
      " [0.33785537 0.33149469 0.33064995]\n",
      " [0.33791775 0.33146924 0.33061301]\n",
      " [0.33973    0.33075307 0.32951693]\n",
      " [0.33956777 0.33081519 0.32961705]\n",
      " [0.3357721  0.33232265 0.33190524]\n",
      " [0.33964141 0.33078409 0.3295745 ]\n",
      " [0.33792024 0.33146499 0.33061477]\n",
      " [0.33363495 0.33320846 0.33315659]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33889373 0.33107617 0.3300301 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33479604 0.33272743 0.33247653]\n",
      " [0.3341518  0.3329944  0.3328538 ]\n",
      " [0.33398108 0.33306511 0.3329538 ]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33576452 0.3323258  0.33190968]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33405523 0.3330344  0.33291037]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33275592 0.33602489 0.33121919]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33317336 0.33408325 0.33274339]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.332014   0.33942393 0.32856207]\n",
      " [0.33178879 0.3404431  0.3277681 ]\n",
      " [0.33225728 0.33831645 0.32942627]\n",
      " [0.33258048 0.33683453 0.33058499]\n",
      " [0.33285916 0.33554672 0.33159412]\n",
      " [0.33282107 0.33572326 0.33145566]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33331769 0.33341727 0.33326504]\n",
      " [0.33330789 0.33345283 0.33323928]\n",
      " [0.33332729 0.33336172 0.33331099]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33328461 0.3335621  0.33315329]\n",
      " [0.33323339 0.3338022  0.3329644 ]\n",
      " [0.33347098 0.33378058 0.33274844]\n",
      " [0.33320384 0.33394059 0.33285557]\n",
      " [0.33338087 0.33401235 0.33260679]\n",
      " [0.33367033 0.33381067 0.332519  ]\n",
      " [0.3334004  0.33415739 0.33244221]\n",
      " [0.33338173 0.33424801 0.33237026]\n",
      " [0.33310604 0.33439785 0.33249611]\n",
      " [0.33311714 0.33450866 0.3323742 ]\n",
      " [0.33392324 0.33396718 0.33210958]\n",
      " [0.33374336 0.33425387 0.33200277]\n",
      " [0.33408586 0.33396578 0.33194836]\n",
      " [0.3341556  0.33397603 0.33186837]\n",
      " [0.33484676 0.33316281 0.33199044]\n",
      " [0.33396373 0.33433794 0.33169833]\n",
      " [0.33457174 0.33376426 0.33166399]\n",
      " [0.33518145 0.33258829 0.33223026]\n",
      " [0.33463416 0.33387237 0.33149347]\n",
      " [0.33528973 0.3330408  0.33166947]\n",
      " [0.3352945  0.33315948 0.33154602]\n",
      " [0.33575838 0.33235992 0.3318817 ]\n",
      " [0.33551803 0.33245143 0.33203054]\n",
      " [0.33595589 0.33228422 0.33175989]\n",
      " [0.3360691  0.33223695 0.33169395]\n",
      " [0.33611191 0.33221717 0.33167092]\n",
      " [0.33557468 0.33242575 0.33199957]\n",
      " [0.33631263 0.33213686 0.33155051]\n",
      " [0.33614188 0.33220097 0.33165715]\n",
      " [0.33603745 0.3330355  0.33092705]\n",
      " [0.33660185 0.33202082 0.33137733]\n",
      " [0.33549347 0.33245446 0.33205207]\n",
      " [0.33423938 0.33295811 0.3328025 ]\n",
      " [0.33451231 0.33284502 0.33264266]\n",
      " [0.33616896 0.3321849  0.33164613]\n",
      " [0.33635807 0.33210946 0.33153247]\n",
      " [0.33427508 0.33294332 0.3327816 ]\n",
      " [0.3363207  0.33212266 0.33155664]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33392166 0.33308973 0.33298861]\n",
      " [0.33547132 0.3324579  0.33207078]\n",
      " [0.33449673 0.33285148 0.33265179]\n",
      " [0.33413369 0.3330019  0.33286441]\n",
      " [0.33400352 0.33305582 0.33294066]\n",
      " [0.33450177 0.33284939 0.33264884]\n",
      " [0.33371744 0.3331743  0.33310826]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.3331098  0.33438028 0.33250992]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33330942 0.33344565 0.33324493]\n",
      " [0.3326194  0.33665527 0.33072534]\n",
      " [0.33280105 0.335816   0.33138294]\n",
      " [0.33312396 0.33822622 0.32864982]\n",
      " [0.33253676 0.33703572 0.33042752]\n",
      " [0.33307955 0.33452149 0.33239896]\n",
      " [0.33222795 0.33845036 0.32932169]\n",
      " [0.3323467  0.33879854 0.32885475]\n",
      " [0.33274873 0.33877132 0.32847995]\n",
      " [0.33270154 0.33627625 0.33102221]\n",
      " [0.33219731 0.33859009 0.3292126 ]\n",
      " [0.33404591 0.33828548 0.32766861]\n",
      " [0.33266418 0.33644873 0.33088709]\n",
      " [0.33207954 0.33912626 0.3287942 ]\n",
      " [0.33230531 0.33809702 0.32959767]\n",
      " [0.33205029 0.33925915 0.32869056]\n",
      " [0.33200527 0.33946352 0.32853121]\n",
      " [0.33754021 0.33511438 0.32734542]\n",
      " [0.33422234 0.33870477 0.32707289]\n",
      " [0.33325353 0.33941958 0.32732689]\n",
      " [0.33332156 0.33946218 0.32721626]\n",
      " [0.3367426  0.33633861 0.3269188 ]\n",
      " [0.3369608  0.33619298 0.32684622]\n",
      " [0.34093922 0.33028265 0.32877813]\n",
      " [0.33987636 0.33258166 0.32754199]\n",
      " [0.33482522 0.33874247 0.32643231]\n",
      " [0.33783069 0.33557462 0.32659468]\n",
      " [0.34130576 0.330124   0.32857025]\n",
      " [0.33931613 0.33388364 0.32680023]\n",
      " [0.33976899 0.33072299 0.32950802]\n",
      " [0.33922228 0.33421595 0.32656177]\n",
      " [0.34140072 0.33007885 0.32852043]\n",
      " [0.34113842 0.33137802 0.32748357]\n",
      " [0.34194646 0.32987303 0.32818051]\n",
      " [0.33900819 0.33102182 0.32996999]\n",
      " [0.34161388 0.32998992 0.3283962 ]\n",
      " [0.33871997 0.33113536 0.33014467]\n",
      " [0.3392595  0.33091983 0.32982068]\n",
      " [0.33751269 0.33161576 0.33087155]\n",
      " [0.34189575 0.32987441 0.32822983]\n",
      " [0.33975204 0.33072179 0.32952617]\n",
      " [0.33657226 0.33199055 0.33143718]]\n"
     ]
    }
   ],
   "source": [
    "#ACTIVATION FUNCTIONS\n",
    "\n",
    "def create_data(points,classes):\n",
    "    X=np.zeros((points*classes,2))\n",
    "    y=np.zeros(points*classes,dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix=range(points*class_number,points*(class_number+1))\n",
    "        r=np.linspace(0.0,1,points)\n",
    "        t=np.linspace(class_number*4,(class_number+1)*4,points)+np.random.randn(points)*0.2\n",
    "        X[ix]=np.c_[r*np.sin(t*2.5),r*np.cos(t*2.5)]\n",
    "        y[ix]=class_number\n",
    "    return X,y\n",
    "X,y=create_data(100,3)\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights=0.1*np.random.randn(n_inputs,n_neurons) #this is reversed to eliminate transpose later on\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.output=np.maximum(0,inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self,inputs):\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        probabilities=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probabilities\n",
    "\n",
    "dense1=Layer_Dense(2,3)\n",
    "Activation1=Activation_ReLU()\n",
    "dense2=Layer_Dense(3,3)\n",
    "Activation2=Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "Activation1.forward(dense1.output)\n",
    "dense2.forward(Activation1.output)\n",
    "Activation2.forward(dense2.output)\n",
    "\n",
    "print(Activation2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOSS FUNCTION'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
